# Практическое задание 2: Исследование атак на модели ИИ. Fast Gradient Sign Method (FGSM)


## Цель задания:

Познакомиться с одной из популярных атак на системы машинного обучения — атакой Fast Gradient
Sign Method (FGSM).

## Задачи:

1. Загрузить ранее обученную модель на датасете MNIST.
2. Изучить теоретические основы FGSM.
3. Реализовать атаку FGSM и сгенерировать противоречивые примеры.
4. Оценить точность модели на противоречивых примерах и сравнить с результатами на обычных данных.

## Вывод:

В ходе выполнения практической работы была проанализирована атака Fast Gradient Sign Method (FGSM), направленная на модели машинного обучения. Уровень шума, определяемый параметром epsilon и установленный на значении 0.1, задавал степень изменений, вносимых в исходные данные. Эксперимент проводился на модели, предварительно обученной на наборе данных MNIST, с целью формирования противоречивых примеров, которые способны дезориентировать модель.

Изначальные показатели модели:
До использования FGSM модель достигала точности предсказаний на тестовых данных в 97%. Однако после применения атаки и генерации противоречивых примеров этот показатель снизился до 9%. Такой результат демонстрирует, насколько модель чувствительна к небольшим, но специально сформированным изменениям во входных данных, увеличивающим вероятность ошибок в предсказаниях. Значительное снижение точности подчеркивает эффективность метода FGSM в создании вводящих в заблуждение примеров.

Выводы и значимость:
Эти данные акцентируют внимание на необходимости изучения устойчивости моделей к подобным атакам, особенно в сферах, где надежность играет решающую роль. Создание защитных механизмов, таких как подходы к повышению сопротивляемости противоречивым примерам, становится одной из важнейших задач для обеспечения стабильной работы современных систем машинного обучения.
## Студент

Сапов Александр Дмитриевич
Группа ББМО-02-23
